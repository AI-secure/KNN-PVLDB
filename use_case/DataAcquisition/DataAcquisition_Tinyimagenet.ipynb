{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from utils import *\n",
    "from models.resnetF import *\n",
    "from plot_cnn import *\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import torchvision.models as models\n",
    "import torch, torchvision\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision.datasets as datasets\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from shap_utils import *\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "import copy\n",
    "#hyper parameters\n",
    "batch_size = 100\n",
    "epochs = 30\n",
    "# train_size = 1000\n",
    "# test_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "2500 2500 10000 95000\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "data_transforms = { 'train': transforms.Compose([transforms.ToTensor()]),\n",
    "                    'val'  : transforms.Compose([transforms.ToTensor(),]) }\n",
    "data_dir = \"data/tiny-imagenet-200\"\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=64)\n",
    "              for x in ['train', 'val']}\n",
    "print(len(image_datasets['train']))\n",
    "middle_dataset, pre_dataset = torch.utils.data.random_split(image_datasets['train'], [5000, 95000])\n",
    "train_dataset, cal_dataset = torch.utils.data.random_split(middle_dataset, [2500, 2500])\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=64)\n",
    "test_dataloader = torch.utils.data.DataLoader(image_datasets['val'], batch_size=batch_size, shuffle=True, num_workers=64)\n",
    "cal_dataloader = torch.utils.data.DataLoader(cal_dataset, batch_size=batch_size, shuffle=True, num_workers=64)\n",
    "pre_dataloader = torch.utils.data.DataLoader(pre_dataset, batch_size=batch_size, shuffle=True, num_workers=64)\n",
    "\n",
    "# pre_dataloader = dataloaders['val']\n",
    "cal_size = len(cal_dataset)\n",
    "\n",
    "train_size = len(train_dataset)\n",
    "test_size = len(image_datasets['val'])\n",
    "pre_size = len(pre_dataset)\n",
    "print(train_size, cal_size, test_size, pre_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tiny_train(phase, model, device, dataloaders, dataset_sizes, optimizer, criterion, scheduler, epochs=1):   \n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1, epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        if phase == 'train':\n",
    "            scheduler.step()\n",
    "            model.train()  # Set model to training mode\n",
    "        else:\n",
    "            model.eval()   # Set model to evaluate mode\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        # Iterate over data.\n",
    "        for i,(inputs, labels) in enumerate(dataloaders):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                *_, outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        print(\"\\repochs: {}/{}, Loss: {}.\".format(epoch+1, epochs, loss.item() * inputs.size(0)), end=\"\")\n",
    "  \n",
    "        epoch_loss = running_loss / dataset_sizes\n",
    "        epoch_acc = running_corrects.double() / dataset_sizes\n",
    "        if phase == 'train':\n",
    "            avg_loss = epoch_loss\n",
    "            t_acc = epoch_acc\n",
    "        elif phase == 'val':\n",
    "            val_loss = epoch_loss\n",
    "            val_acc = epoch_acc    \n",
    "    if phase == 'train':\n",
    "        print('Train Loss: {:.4f} Acc: {:.4f}'.format(avg_loss, t_acc))\n",
    "    elif phase == 'val':\n",
    "        print('Val Loss: {:.4f} Acc: {:.4f}'.format(val_loss, val_acc))\n",
    "        return val_acc\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "----------\n",
      "epochs: 1/30, Loss: 542.0825004577637.Epoch 2/30\n",
      "----------\n",
      "epochs: 2/30, Loss: 491.0949230194092.Epoch 3/30\n",
      "----------\n",
      "epochs: 3/30, Loss: 443.6898708343506.Epoch 4/30\n",
      "----------\n",
      "epochs: 4/30, Loss: 381.7734479904175.Epoch 5/30\n",
      "----------\n",
      "epochs: 5/30, Loss: 350.2703905105591.Epoch 6/30\n",
      "----------\n",
      "epochs: 6/30, Loss: 362.526273727417.Epoch 7/30\n",
      "----------\n",
      "epochs: 7/30, Loss: 350.4526138305664.Epoch 8/30\n",
      "----------\n",
      "epochs: 8/30, Loss: 338.5174036026001.Epoch 9/30\n",
      "----------\n",
      "epochs: 9/30, Loss: 334.5339059829712.Epoch 10/30\n",
      "----------\n",
      "epochs: 10/30, Loss: 336.1944913864136.Epoch 11/30\n",
      "----------\n",
      "epochs: 11/30, Loss: 334.90304946899414.Epoch 12/30\n",
      "----------\n",
      "epochs: 12/30, Loss: 343.7199354171753.Epoch 13/30\n",
      "----------\n",
      "epochs: 13/30, Loss: 336.76133155822754.Epoch 14/30\n",
      "----------\n",
      "epochs: 14/30, Loss: 342.15545654296875.Epoch 15/30\n",
      "----------\n",
      "epochs: 15/30, Loss: 342.21439361572266.Epoch 16/30\n",
      "----------\n",
      "epochs: 16/30, Loss: 342.50268936157227.Epoch 17/30\n",
      "----------\n",
      "epochs: 17/30, Loss: 343.0098533630371.Epoch 18/30\n",
      "----------\n",
      "epochs: 18/30, Loss: 334.33966636657715.Epoch 19/30\n",
      "----------\n",
      "epochs: 19/30, Loss: 335.45026779174805.Epoch 20/30\n",
      "----------\n",
      "epochs: 20/30, Loss: 337.8366470336914.Epoch 21/30\n",
      "----------\n",
      "epochs: 21/30, Loss: 331.68976306915283.Epoch 22/30\n",
      "----------\n",
      "epochs: 22/30, Loss: 352.9198884963989.Epoch 23/30\n",
      "----------\n",
      "epochs: 23/30, Loss: 335.23430824279785.Epoch 24/30\n",
      "----------\n",
      "epochs: 24/30, Loss: 344.0568208694458.Epoch 25/30\n",
      "----------\n",
      "epochs: 25/30, Loss: 333.98241996765137.Epoch 26/30\n",
      "----------\n",
      "epochs: 26/30, Loss: 327.84643173217773.Epoch 27/30\n",
      "----------\n",
      "epochs: 27/30, Loss: 335.429310798645.Epoch 28/30\n",
      "----------\n",
      "epochs: 28/30, Loss: 338.13531398773193.Epoch 29/30\n",
      "----------\n",
      "epochs: 29/30, Loss: 345.29709815979004.Epoch 30/30\n",
      "----------\n",
      "epochs: 30/30, Loss: 335.38174629211426.Train Loss: 3.3652 Acc: 0.4916\n",
      "Epoch 1/1\n",
      "----------\n",
      "epochs: 1/1, Loss: 503.8386821746826.Val Loss: 4.9660 Acc: 0.0632\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0632, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic=True\n",
    "from models.resnetF import *\n",
    "device = torch.device('cuda')\n",
    "model = resnet18(pretrained=True)\n",
    "model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 200)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "model = model.to(device)\n",
    "epochs = 30\n",
    "tiny_train(\"train\", model, device, train_dataloader, train_size, optimizer, criterion, scheduler, epochs)\n",
    "tiny_train(\"val\", model, device, test_dataloader, test_size, optimizer, criterion, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 12288) (2500,) (2500, 512)\n",
      "(2500, 12288) (2500,) (2500, 512)\n"
     ]
    }
   ],
   "source": [
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "deep_f_train = []\n",
    "y_train = []\n",
    "x_train = []\n",
    "deep_f_cal = []\n",
    "y_cal = []\n",
    "x_cal = []\n",
    "for i,(X, y) in enumerate(train_dataloader):\n",
    "    X = X.to(device).float()\n",
    "    out_fc1, y_pre = model(X)\n",
    "    deep_f_train.append(out_fc1.view(out_fc1.size(0), -1).cpu().detach().numpy())\n",
    "    x_train.append(X.view(X.size(0), -1).cpu().detach().numpy())\n",
    "    y_train.append(y.numpy())\n",
    "\n",
    "deep_f_train = np.concatenate(deep_f_train) # deep features are not normalized\n",
    "y_train = np.concatenate(y_train)\n",
    "x_train = np.concatenate(x_train)\n",
    "print(x_train.shape, y_train.shape, deep_f_train.shape)\n",
    "\n",
    "for i,(X, y) in enumerate(cal_dataloader):\n",
    "#     print(i)\n",
    "    X = X.to(device).float()\n",
    "    out_fc1, y_pre = model(X)\n",
    "    deep_f_cal.append(out_fc1.view(out_fc1.size(0), -1).cpu().detach().numpy())\n",
    "    x_cal.append(X.view(X.size(0), -1).cpu().detach().numpy())\n",
    "    y_cal.append(y.numpy())\n",
    "deep_f_cal = np.concatenate(deep_f_cal) # deep features are not normalized\n",
    "y_cal = np.concatenate(y_cal)\n",
    "x_cal = np.concatenate(x_cal)\n",
    "print(x_cal.shape, y_cal.shape, deep_f_cal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbour number: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbour number: 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kmin = 1\n",
    "kmax = 10\n",
    "kinterval = 1\n",
    "fc1_knn_values = [[] for _ in range(math.ceil((kmax-kmin)/kinterval))] # deep features\n",
    "loo_fc1_knn_values = [[] for _ in range(math.ceil((kmax-kmin)/kinterval))] # deep features\n",
    "\n",
    "fc1_scores = [[] for _ in range(math.ceil((kmax-kmin)/kinterval))]\n",
    "\n",
    "for i, k in enumerate(range(kmin, kmax, kinterval)):\n",
    "    print(\"neighbour number:\", k)\n",
    "    loo_fc1_knn_values[i],fc1_scores[i], fc1_false = loo_knn_shapley(k, deep_f_train, deep_f_cal, y_train, y_cal)\n",
    "    fc1_knn_values[i],*_ = old_knn_shapley(k, deep_f_train, deep_f_cal, y_train, y_cal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8.07750307e-06 -1.36472570e-05  2.08875064e-05  2.71985499e-05\n",
      "  2.33321028e-06  5.17637251e-04 -8.14243583e-07  3.73426756e-05\n",
      "  1.89738024e-04  4.88869666e-04]\n",
      "(10000,)\n",
      "(95000,)\n"
     ]
    }
   ],
   "source": [
    "print(fc1_knn_values[0][:10])\n",
    "x_heldout = []\n",
    "y_heldout = []\n",
    "for i,(X, y) in enumerate(test_dataloader):\n",
    "    x_heldout.append(X.view(X.size(0), -1).cpu().detach().numpy())\n",
    "    y_heldout.append(y.numpy())\n",
    "y_heldout = np.concatenate(y_heldout)\n",
    "x_heldout = np.concatenate(x_heldout)  \n",
    "print(y_heldout.shape)\n",
    "\n",
    "x_pre = []\n",
    "y_pre = []\n",
    "for i,(X, y) in enumerate(pre_dataloader):\n",
    "    x_pre.append(X.view(X.size(0), -1).cpu().detach().numpy())\n",
    "    y_pre.append(y.numpy())\n",
    "x_pre = np.concatenate(x_pre)\n",
    "y_pre = np.concatenate(y_pre)  \n",
    "print(y_pre.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# save data\n",
    "print(x_train.shape, x_test.shape)\n",
    "\n",
    "store_data = './exp_data/DS_tinyimagenet/'\n",
    "f = open(store_data+'data.pkl', 'wb')\n",
    "data_write = {'x_trian': x_train, 'y_train': y_train, 'x_test':x_test, 'y_test': y_test, \n",
    "              'x_heldout': x_heldout, 'y_heldout': y_heldout}\n",
    "pickle.dump(data_write, f)\n",
    "f.close() \n",
    "\n",
    "f = open(store_data+'SV.pkl', 'wb')\n",
    "data_write = {'kmin': kmin, 'kmax': kmax, 'kinterval': kinterval, 'knn_values': fc1_knn_values}\n",
    "pickle.dump(data_write, f)\n",
    "f.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8.07750307e-06 -1.36472570e-05  2.08875064e-05  2.71985499e-05\n",
      "  2.33321028e-06  5.17637251e-04 -8.14243583e-07  3.73426756e-05\n",
      "  1.89738024e-04  4.88869666e-04]\n",
      "[ 8.84888276e-04 -1.49505331e-03  2.28822067e-03  2.97959378e-03\n",
      "  2.55602555e-04  5.67070208e-02 -8.92001643e-05  4.09088001e-03\n",
      "  2.07857492e-02  5.35555396e-02]\n",
      "[0.         0.         0.         0.         0.         0.05479966\n",
      " 0.         0.         0.         0.05479966]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "knn_values: 2500\n",
      "loo_knn_values: 2500\n",
      "knn_values: 2500\n",
      "loo_knn_values: 2500\n",
      "finish fit\n",
      "finish fit\n",
      "finish fit\n",
      "finish fit\n"
     ]
    }
   ],
   "source": [
    "loo_pre_score = []\n",
    "tmc_pre_score = []\n",
    "knn_pre_score = [[] for _ in range(math.ceil((kmax-kmin)/kinterval))]\n",
    "loo_knn_pre_score = [[] for _ in range(math.ceil((kmax-kmin)/kinterval))]\n",
    "knn_pre_idx = [[] for _ in range(math.ceil((kmax-kmin)/kinterval))]\n",
    "loo_knn_pre_idx = [[] for _ in range(math.ceil((kmax-kmin)/kinterval))]\n",
    "\n",
    "print(fc1_knn_values[0][:10])\n",
    "\n",
    "for i in range(len(fc1_knn_values)):\n",
    "    fc1_knn_values[i] = fc1_knn_values[i] / np.linalg.norm(fc1_knn_values[i])\n",
    "    loo_fc1_knn_values[i] = loo_fc1_knn_values[i] / np.linalg.norm(loo_fc1_knn_values[i])\n",
    "print(fc1_knn_values[0][:10])\n",
    "\n",
    "\n",
    "print(loo_fc1_knn_values[0][:10])\n",
    "\n",
    "print(loo_fc1_knn_values[0][-10:])\n",
    "\n",
    "\n",
    "knn_values_idx = [[] for _ in range(math.ceil((kmax-kmin)/kinterval))]\n",
    "loo_knn_values_idx = [[] for _ in range(math.ceil((kmax-kmin)/kinterval))]\n",
    "\n",
    "for i in range(len(fc1_knn_values)):\n",
    "    knn_values_idx[i] = np.where(fc1_knn_values[i] > -100.0000)[0]\n",
    "    loo_knn_values_idx[i] = np.where(loo_fc1_knn_values[i] > -100.0000)[0]\n",
    "\n",
    "    print(\"knn_values:\", len(knn_values_idx[i]))\n",
    "    print(\"loo_knn_values:\", len(loo_knn_values_idx[i]))\n",
    "\n",
    "clf_knn = [[] for _ in range(math.ceil((kmax-kmin)/kinterval))]\n",
    "clf_loo_knn = [[] for _ in range(math.ceil((kmax-kmin)/kinterval))]\n",
    "\n",
    "for i in range(len(fc1_knn_values)):\n",
    "#     clf_knn[i] = return_model('mlpreg')\n",
    "    clf_knn[i] = return_model('RandomForestReg')\n",
    "\n",
    "    clf_knn[i].fit(x_train[knn_values_idx[i]], fc1_knn_values[i][knn_values_idx[i]])\n",
    "    print(\"finish fit\")\n",
    "\n",
    "for i in range(len(loo_fc1_knn_values)):\n",
    "#     clf_loo_knn[i] = return_model('RandomForestReg')\n",
    "    clf_loo_knn[i] = return_model('mlpreg')\n",
    "    clf_loo_knn[i].fit(x_train[loo_knn_values_idx[i]], loo_knn_values_idx[i][loo_knn_values_idx[i]])\n",
    "    print(\"finish fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00020123  0.00614738  0.00098427  0.01952971  0.00651056  0.01658965\n",
      "  0.00782489  0.00945298  0.00250302 -0.0022188 ]\n",
      "[ 756.44348538  442.79842198 1258.99345873  513.39372426 1485.12419292\n",
      "  273.11735032  941.82725226  562.57933618  592.16990526 1016.71952644]\n",
      "[11012 88915 67643 50583 74800  7092 71058 20305 10697 89971 83910 57845\n",
      " 92481  2724    19 83934 47234  5729 57580 22259 35155 94957 30791 11924\n",
      " 13643 65170 56372 33673 34971 68403 47570 74267 36238 60901 62692 23296\n",
      " 14512 51752 38338 11235 57898  8342  5297  5668 81950 93005  2853 24983\n",
      "  1922 87564 50153 39214 30588  2846 22086 47652 54387 18881 86117  5153\n",
      " 24385  9410 78515 27356 84880 81084 86378  2768 53834 77068 79023 60339\n",
      " 16859 39265 36329  7589 92802 50437 60759 57976 80103 87139 25904 55391\n",
      " 75316 67717 19463 86417 16282 24549 87351 49591 31329 75232 66612 87513\n",
      " 77865 69121 36916 19369]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(knn_values_idx)):\n",
    "    knn_pre_score[i] = clf_knn[i].predict(x_pre)\n",
    "    knn_pre_idx[i]=np.argsort(knn_pre_score[i])\n",
    "\n",
    "for i in range(len(loo_fc1_knn_values)):\n",
    "    loo_knn_pre_score[i] = clf_loo_knn[i].predict(x_pre)\n",
    "    loo_knn_pre_idx[i]=np.argsort(loo_knn_pre_score[i])\n",
    "    \n",
    "print((knn_pre_score[0][:10]))\n",
    "print((loo_knn_pre_score[0][:10]))\n",
    "print(knn_pre_idx[0][-100:])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "loo_pre_score = []\n",
    "tmc_pre_score = []\n",
    "knn_pre_score = [[] for _ in range(math.ceil((kmax-kmin)/kinterval))]\n",
    "loo_knn_pre_score = [[] for _ in range(math.ceil((kmax-kmin)/kinterval))]\n",
    "knn_pre_idx = [[] for _ in range(math.ceil((kmax-kmin)/kinterval))]\n",
    "loo_knn_pre_idx = [[] for _ in range(math.ceil((kmax-kmin)/kinterval))]\n",
    "for i in range(len(fc1_knn_values)):\n",
    "    clf_knn[i] = return_model('RandomForestReg')\n",
    "    clf_knn[i].fit(x_train[knn_values_idx[i]], fc1_knn_values[i][knn_values_idx[i]])\n",
    "    \n",
    "for i in range(len(knn_values_idx)):\n",
    "    iv = int(x_pre.shape[0]*0.01)\n",
    "    for item  in range(0, x_pre.shape[0], iv):\n",
    "        print(item)\n",
    "        knn_pre_score[i] = clf_knn[i].predict(x_pre[item:(item+iv)])\n",
    "# knn_pre_idx[i]=np.argsort(knn_pre_score[i])\n",
    "#     loo_knn_pre_score[i] = clf_loo_knn[i].predict(x_pre)\n",
    "#     loo_knn_pre_idx[i]=np.argsort(loo_knn_pre_score[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.17172956466674805, 0.26072579622268677, 0.1966152191162109, 0.16916394233703613, 0.12126377224922182, 0.021961569786071777, 0.27344438433647156, -0.38717180490493774, -0.759654700756073, -0.15183240175247192]\n",
      "[ 0.00609461  0.00925305  0.00697779  0.00600356  0.0043036   0.00077941\n",
      "  0.00970443 -0.01374056 -0.02695982 -0.00538847]\n",
      "finish fit\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"exp_data/DA_tinyimagenet/inf_fun.pkl\", \"rb\") as f:\n",
    "    d = pickle.load(f)\n",
    "    inf_values = d[\"loo\"]\n",
    "    \n",
    "print(inf_values[:10])\n",
    "inf_values = inf_values / np.linalg.norm(inf_values)\n",
    "print(inf_values[:10])\n",
    "\n",
    "\n",
    "inf_pre_score = []\n",
    "inf_values_idx = np.where(inf_values > -100.0000)[0]\n",
    "clf_inf = return_model('RandomForestReg')\n",
    "\n",
    "clf_inf.fit(x_train[inf_values_idx], inf_values[inf_values_idx])\n",
    "print(\"finish fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00023628  0.00318097  0.0027922  -0.0069073  -0.00024983  0.00185853\n",
      "  0.00483222 -0.00094235  0.0038929   0.0034466 ]\n",
      "[ 0.00609461  0.00925305  0.00697779 ... -0.01901985  0.0040502\n",
      "  0.0087479 ]\n"
     ]
    }
   ],
   "source": [
    "inf_pre_score = clf_inf.predict(x_pre)\n",
    "print(inf_pre_score[:10])\n",
    "inf_pre_idx = np.argsort(inf_pre_score)\n",
    "print(inf_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(avg_knn_values)\n",
    "from plot_resnet import *\n",
    "seed = 48\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic=True\n",
    "\n",
    "sx_train = torch.from_numpy(x_train).contiguous().view(-1, 3,64,64)\n",
    "sy_train = torch.from_numpy(y_train).view(-1,).long()\n",
    "print(\"train_size:\", sx_train.shape)\n",
    "sx_test = torch.from_numpy(x_heldout).contiguous().view(-1, 3,64,64)\n",
    "sy_test = torch.from_numpy(y_heldout).view(-1,).long()\n",
    "print(\"test_size:\", sx_test.shape)\n",
    "sx_pre = torch.from_numpy(x_pre).contiguous().view(-1, 3,64,64)\n",
    "sy_pre = torch.from_numpy(y_pre).view(-1,).long()\n",
    "print(\"test_size:\", sx_pre.shape)\n",
    "\n",
    "plot_resnet_acq(knn_pre_idx, loo_knn_pre_idx, [inf_pre_idx], kmin, kmax, kinterval, sx_train, sy_train, sx_test, sy_test, sx_pre, sy_pre, batch_size, \n",
    "            epochs=10, HtoL=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
